{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "725a525c",
   "metadata": {},
   "source": [
    "<center><img src='./Figs/cs-logo.png' width=200></center>\n",
    "\n",
    "\n",
    "\n",
    "<h6><center></center></h6>\n",
    "\n",
    "<h1>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center> Mention IA - CentraleSupélec - Visual Recognition  </center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c279a",
   "metadata": {},
   "source": [
    "# Partie 1 : Video Magnification\n",
    "\n",
    "\n",
    "Cette première partie du TP consistera à implémenter une version d'un algorithme d'amélioration de la qualité d'une vidéo qui se base sur des mouvements Eulériens. Il s'agit de l'approche décrite dans le papier de [Wu el al](http://people.csail.mit.edu/mrub/papers/vidmag.pdf) en 2012 lors de la conference SIGGRAPH. Prenez le temps de lire rapidement ce papier. Vous pouvez aussi consulter la page web du projet [ici](http://people.csail.mit.edu/mrub/evm/) et [ici](http://people.csail.mit.edu/mrub/vidmag/#publications).\n",
    "\n",
    "Vous pouvez aussi regarder la vidéo qui explique le principe de l'approche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869152c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"350\"\n",
       "            src=\"https://www.youtube.com/embed/ONZcjs1Pjmk\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2de9fea0790>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://www.youtube.com/embed/ONZcjs1Pjmk', width=700, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24a7f0",
   "metadata": {},
   "source": [
    "La méthode proposée permet d'amplifier les couleurs et les mouvements d'une simple vidéo pour nous aider à visualiser les changements imperceptibles du monde qui nous entoure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aab9ba",
   "metadata": {},
   "source": [
    "## 1. 1. Récupération des données\n",
    "\n",
    "Un ensemble de vidéos est disponible dans le répertoire [data](./data). \n",
    "\n",
    "Votre premier travail consistera à écrire une fonction permettant de charger et de lire une vidéo et de la transformer en une liste de frames (array numpy).\n",
    "\n",
    "Vous pourrez pour cela utiliser la fonction `cv2.VideoCapture` en vous inspirant de cette [documentation](https://docs.opencv.org/4.7.0/dd/d43/tutorial_py_video_display.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367c0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_video(vidFile):\n",
    "    '''\n",
    "    Reads the video\n",
    "    :param vidFile: Video file\n",
    "    :return: video sequence, frame rate, width & height of video frames\n",
    "    '''\n",
    "    print('Load video')\n",
    "    \n",
    "    # To complete\n",
    "    video = cv2.VideoCapture(vidFile)\n",
    "    # store video as list of frames, should be an np array\n",
    "    video_stack = []\n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read()\n",
    "        if ret:\n",
    "            video_stack.append(frame)\n",
    "        else:\n",
    "            break\n",
    "    fr = video.get(cv2.CAP_PROP_FPS)\n",
    "    vidWidth = video.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    vidHeight = video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "\n",
    "    return video_stack, fr, vidWidth, vidHeight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da8d33",
   "metadata": {},
   "source": [
    "Tester votre fonction sur une des vidéos fournies dans le répertoire [data](./data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b018754b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load video\n"
     ]
    }
   ],
   "source": [
    "video_stack, fr, vidWidth, vidHeight = load_video(\"./data/baby.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf21f439",
   "metadata": {},
   "source": [
    "Pour ceux qui auraient des difficultés avec cette fonction. Vous trouverez dans le répertoire [./data/Images](./data/Images) les images correspondants à la vidéo `baby.mp4` qui est composée de 300 frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93884488",
   "metadata": {},
   "source": [
    "Dans la suite, vous aurez aussi besoin d'une fonction qui permet de sauvegarder une vidéo. Cette fonction vous est donnée ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcf84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "\n",
    "def save_video(video_tensor, fps, name):\n",
    "    '''\n",
    "    Creates a new video for the output\n",
    "    :param video_tensor: filtered video sequence\n",
    "    :param fps: frame rate of original video\n",
    "    :param name: output video name\n",
    "    '''\n",
    "    print('Save video')\n",
    "    if platform.system()=='Linux':\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    else:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'PIM1')\n",
    "    [height, width] = video_tensor[0].shape[0:2]\n",
    "    writer = cv2.VideoWriter(name+\"Out.avi\", fourcc, fps, (width, height), 1)\n",
    "    for i in range(video_tensor.shape[0]):\n",
    "        writer.write(cv2.convertScaleAbs(video_tensor[i]))\n",
    "    writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70799d",
   "metadata": {},
   "source": [
    "## 1.2 Conversion des images couleur dans l'espace YIQ\n",
    "\n",
    "La première étape de l'algorithme de magnification consiste à convertir les images RVB composant la vidéo d'entrée en YIQ. La composante Y représente la luminosité, I et Q représentent la chrominance (c'est-à-dire l'information sur les couleurs). Cet espace colorimétrique permet de manipuler les couleurs d'une image indépendamment de sa luminosité. La conversion de RVB en YIQ peut être effectuée à l'aide de la formule suivante\n",
    "\n",
    "<center><img src='./Figs/rgbToYiq.png' width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9633b",
   "metadata": {},
   "source": [
    "Compléter les fonctions suivantes permettant de convertir une image de l'espace colorimétrique RGB (ou BGR) à YIQ et inversement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51488470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert RBG to YIQ\n",
    "def rgb2yiq(src):\n",
    "    # To complete\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a23cc20",
   "metadata": {},
   "source": [
    "On a aussi la conversion entre YIQ et RGB\n",
    "<center><img src='./Figs/yiq2rgb.svg' width=400></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3313682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert YIQ to RBG\n",
    "def yiq2rgb(src):\n",
    "    # to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8eed0",
   "metadata": {},
   "source": [
    "Tester vos fonctions sur une image provenant de la video `baby.mp4` et afficher les images obtenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18103cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"./data/Images/baby_0.jpg\",1)\n",
    "# To complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c077d",
   "metadata": {},
   "source": [
    "## Magnification des couleurs\n",
    "### 1.3 Construction d'une pyramide de gaussiennes.\n",
    "\n",
    "\n",
    "Pour magnifier les couleurs, il faut construire une pyramide de gaussiennes de $l$ niveaux pour chaque image de la vidéo d'entrée. Pour cela,  il faudra prendre la version YIQ de chaque image originale et la sous-échantillonner d'un facteur 2 pour chaque nouveau niveau de la pyramide.\n",
    "\n",
    "La première étape du sous-échantillonnage consiste donc à appliquer à l'image une convolution 2D avec le noyau gaussien 5x5 suivant :\n",
    "\n",
    "$$G_{k} = \\frac{1}{256}\\begin{bmatrix}1 & 4 & 6 & 4 & 1\\\\4 & 16 & 24 & 16 & 4\\\\6 & 24 & 36 & 24 & 6\\\\4 & 16 & 24 & 16 & 4\\\\1 & 4 & 6 & 4 & 1\\end{bmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c4c51",
   "metadata": {},
   "source": [
    "Une fois la convolution effectuée, il ne faut conserver qu'un pixel sur deux dans les lignes et les colonnes de l'image, ce qui nous donne une image sous-échantillonnée dont la hauteur et la largeur sont divisées par 2 par rapport au niveau précédent de la pyramide. Nous effectuons ce processus $l -1 $ fois."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267736ce",
   "metadata": {},
   "source": [
    "<center><img src='./Figs/gaussianpyramid.png' width=400></center>\n",
    "Image source : [here](https://iipimage.sourceforge.io/documentation/images/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837ff32",
   "metadata": {},
   "source": [
    "Ecrivez la fonction qui prend en entrée une image (dans le bon espace colorimétrique) et qui renvoie une pyramide de gaussiennes de $l$ niveaux. Pour cela, vous pourrez utiliser les fonctions de la bibliothèque OpenCv et notamment la fonction [`pyrDown`](https://docs.opencv.org/4.7.0/d4/d1f/tutorial_pyramids.html) déjà vue en Lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gaussian_pyramid(img,levels):\n",
    "    # to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b632e",
   "metadata": {},
   "source": [
    "Tester votre fonction sur l'image [baby_0.jpg ](./data/Images/baby_0.jpg) avec une pyramide de 5 niveaux. Vous pourrez par exemple tester la fonction sur l'image originale en RGB puis ensuite sur l'image transformée dans l'espace colorimétrique YIQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e175271",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"./data/Images/baby_0.jpg\",1)\n",
    "# To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55c062",
   "metadata": {},
   "source": [
    "Pour les étapes suivantes, vous ne conserverez que le dernier niveau de la pyramide. Plus précisemment, vous allez utiliser la version sur-échantillonnée du dernier niveau de la pyramide. Pour obtenir cette image suréchantillonnée, vous prendrez le dernier niveau de la pyramide, ajouterez un pixel fixé à 0 tous les deux pixels dans les lignes et les colonnes de l'image sous-échantillonnée et appliquerez $ 4 \\times G_k$. Vous devez obtenir une image dont la hauteur et la largeur sont multipliées par 2 par rapport à l'image précédemment suréchantillonnée. Ce processus est effectué $l-1$ fois pour obtenir une image sur-échantillonnée de même dimension que l'image originale. Ici aussi, il est recommandé d'utiliser les fonctions de la bibliothèque OpenCv et notamment la fonction [`pyrUp`](https://docs.opencv.org/4.7.0/d4/d1f/tutorial_pyramids.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37410e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_last_GaussianPyramid(image, level):\n",
    "    # To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7c8a6a",
   "metadata": {},
   "source": [
    "Tester votre fonction sur l'image [baby_0.jpg ](./data/Images/baby_0.jpg) avec une pyramide de 5 niveaux. Vous pourrez par exemple tester la fonction sur l'image originale en RGB puis ensuite sur l'image transformée dans l'espace colorimétrique YIQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1289479",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"./data/Images/baby_0.jpg\",1)\n",
    "# To complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43923116",
   "metadata": {},
   "source": [
    "## 1.4 Filtrage Temporel\n",
    "\n",
    "\n",
    "Cette étape, la plus importante, consiste à filtrer la pyramide de gaussiennes afin de ne conserver que les pixels situés dans la **bande de fréquences** qui nous intéresse $[w_l, w_h]$. Pour cela, nous devons passer du domaine spatial (domaine de l'image) au domaine des fréquences à l'aide d'une transformée de Fourier.\n",
    "\n",
    "Pour cela, vous allez utiliser le package [`fft`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html) de la bibliothèque `numpy`.\n",
    "\n",
    "Si vous n'avez jamais entendu parler de la transformée de Fourier, prenez le temps de lire rapidement ce petit [tutorial](https://homepages.inf.ed.ac.uk/rbf/HIPR2/fourier.htm).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02c76c",
   "metadata": {},
   "source": [
    "Le code ci-dessous vous montre un exemple d'application de cette bibliothèque sur l'image [baby_0.jpg ](./data/Images/baby_0.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "img = cv2.imread(\"./data/Images/baby_0.jpg\",0)\n",
    "rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "axis =0\n",
    "\n",
    "fft = np.fft.fft2(img)\n",
    "\n",
    "# Shift the zero frequency component (i.e., DC component located at the top-left corner) to the center where it will be more easy to analyze\n",
    "\n",
    "fftShift = np.fft.fftshift(fft)\n",
    "\n",
    "# compute the magnitude spectrum of the transform\n",
    "\n",
    "magnitude = 20 * np.log(np.abs(fftShift))\n",
    "\n",
    "# Visualize the obtained image\n",
    "# display the original input image\n",
    "(fig, ax) = plt.subplots(1, 2, )\n",
    "ax[0].imshow(img, cmap=\"gray\")\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "# display the magnitude image\n",
    "ax[1].imshow(magnitude, cmap=\"gray\")\n",
    "ax[1].set_title(\"Magnitude Spectrum\")\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "# show our plots\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80da350",
   "metadata": {},
   "source": [
    "Pour notre problème, il s'agira d'appliquer la transformée de Fourier et de ne conserver que les pixels situés dans la **bande de fréquences** qui nous intéresse $[w_l, w_h]$ ce que l'on pourra faire de la manière suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506231c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "low=0.8333\n",
    "high = 1\n",
    "\n",
    "fft = np.fft.fft(img)\n",
    "fps = 25\n",
    "\n",
    "frequencies = np.fft.fftfreq(img.shape[0], d=1.0/fps)\n",
    "bound_low = (np.abs(frequencies - low)).argmin()\n",
    "bound_high = (np.abs(frequencies - high)).argmin()\n",
    "\n",
    "bound_low = (np.abs(frequencies - low)).argmin()\n",
    "bound_high = (np.abs(frequencies - high)).argmin()\n",
    "fft[:bound_low] = 0\n",
    "fft[bound_high:-bound_high] = 0\n",
    "fft[-bound_low:] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe0bbc",
   "metadata": {},
   "source": [
    "Il faut ensuite appliquer la transformée de Fourier inverse pour obtenir une image filtrée. Le résultat d'une transformée de Fourier inverse est une matrice de nombres complexes, il faudra donc prendre la la partie réelle de chaque nombre de la matrice complexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d614f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultifft = np.fft.ifft(fft, axis=0).real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c31795",
   "metadata": {},
   "source": [
    "A partir des exemples précédents, écrire une fonction `idealTemporalBandpassFilter` qui prend en argument une image, un paramètre `fps`(frame per second), la banque de fréquence voulue (paramètres `low` et `high`) et un argument `axis` qui est fixé par défaut à 0 et qui renvoie la transformée inverse filtrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb669acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idealTemporalBandpassFilter(images,\n",
    "                                fps,\n",
    "                                freq_range,\n",
    "                                axis=0):\n",
    "\n",
    "  # To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca7291",
   "metadata": {},
   "source": [
    "## 1.5 Magnification de l' image et reconstruction.\n",
    "\n",
    "Pour amplifier les couleurs obtenues avec la pyramide filtrée $F_p$, il est nécessaire d'introduire une valeur $\\alpha$ qui va agir comme une **facteur d'amplification**. Pour éviter la création d'artefacts dans l'image de sortie, nous introduisons une autre valeur $A$ qui agira comme un facteur d'atténuation pour les composantes de chrominance de l'image. Nous obtenons notre pyramide filtrée magnifiée $M_{F_P}$ avec une atténuation de la chrominance avec la formule suivante :\n",
    "\n",
    "$$M_{F_{p}} = \\begin{cases}\\alpha F_{p}\\ \\text{if Y component}\\\\ \\alpha A F_{p}\\ \\text{if I or Q component}\\end{cases}$$\n",
    "\n",
    "On peut finalement reconstruire l'image en ajoutant $M_{F_P}$ à la version YIQ de l'image originale et en convertissant l'image résultante en RGB en pensant bien à limiter les valeurs dans l'intervalle $[0,255]$ pour chaque composante. Ce processus devra être répété pour chaque frame de la vidéo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2e374",
   "metadata": {},
   "source": [
    "Ecrire une fonction `filterGaussianPyramids` qui, pour une frame donnée, prend en entrée la pyramide de gaussiennes, la filtre, applique les opérations d'amplification et d'attenuation. Cette fonction prendra aussi en arguments le `fps`, la bande de fréquences et les paramètres `alpha` et `attenuation`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterGaussianPyramids(pyramids,\n",
    "                           fps,\n",
    "                           freq_range,\n",
    "                           alpha,\n",
    "                           attenuation):\n",
    "\n",
    "    # To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de6150",
   "metadata": {},
   "source": [
    "Ecrire une fonction qui reconstruit l'image magnifiée en ajoutant $M_{F_P}$ à la version YIQ de l'image originale et en convertissant l'image résultante en RGB en pensant bien à limiter les valeurs dans l'intervalle $[0,255]$ pour chaque composante. Elle prendra en argument l'image originale et la pyramide de gaussienne filtrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructGaussianImage(image, pyramid):\n",
    "    # To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a47b9d",
   "metadata": {},
   "source": [
    "Tester vos fonctions sur l'image `baby_0.jpg` avec une pyramide de 4 niveaux, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"./data/Images/baby_0.jpg\",1)\n",
    "\n",
    "# To complete\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8210d55a",
   "metadata": {},
   "source": [
    "### 1.6 Application à la vidéo\n",
    "\n",
    "\n",
    "Ecrire le code permettant d'appliquer cela à une vidéo et qui renverra donc la vidéo magnifiée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e243a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_video(video_tensor, levels):\n",
    "    '''\n",
    "    For a given video sequence the function creates a video with\n",
    "    the highest (specified by levels) Gaussian pyramid level\n",
    "    :param video_tensor: Video sequence\n",
    "    :param levels: Specifies the Gaussian pyramid levels\n",
    "    :return: a video sequence where each frame is the downsampled of the original frame\n",
    "    '''\n",
    "   # To complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b63e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "low = 0.4\n",
    "high = 3\n",
    "alpha = 15\n",
    "chromAttenuation = 0.1\n",
    "t, fps, width, height = load_video(\"./data/baby.mp4\")\n",
    "\n",
    "# To complete\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784200ca",
   "metadata": {},
   "source": [
    "\n",
    "## Magnification des mouvements\n",
    "### 1.6 Pyramide de laplaciens\n",
    "\n",
    "Pour la magnification des mouvements, il faut se concentrer sur les bords des objets dans la vidéo. Pour cela, vous allez utiliser la différence entre deux niveaux adjacents de la pyramide gaussienne, c'est-à-dire une pyramide de Laplacien comme illustré sur la figure ci-dessous\n",
    "\n",
    "<center><img src='./Figs/laplacian_pyramid.png' width=500></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb21bf",
   "metadata": {},
   "source": [
    "Ecrire la fonction `create_laplacian_pyramid` qui prend en agurment une image et le niveau voulu $l$ de la pyramide et qui renvoie une pyramide de laplacien de $l$ niveaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_laplacian_pyramid(image, levels):\n",
    "    '''\n",
    "    Builds a Laplace pyramid for an image, i.e. video frame\n",
    "    :param image: Image,  i.e. single video frame\n",
    "    :param levels: Specifies the Laplace pyramid levels\n",
    "    :return: Returns a pyramid of nr. levels images\n",
    "    '''\n",
    "  # To complete\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb1dd4",
   "metadata": {},
   "source": [
    "Ecrire une fonction qui permet de créer la pyramide des laplaciens pour l'ensemble d'une vidéo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918563c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_video_pyramid(video_stack, levels):\n",
    "    '''\n",
    "    Creates a Laplacian pyramid for the whole video sequence\n",
    "    :param video_stack: Video sequence\n",
    "    :param levels: Specifies the Laplace pyramid levels\n",
    "    :return: A two-dimensional array where the first index is used for the pyramid levels\n",
    "    and the second for each video frame\n",
    "    '''\n",
    "    print('Build laplace pyramid')\n",
    "    # To complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7cbd06",
   "metadata": {},
   "source": [
    "### 1.7 Filtre de Butterworth\n",
    "\n",
    "Pour l'étape de filtrage de la magnification des mouvements, il faut un filtre qui tolère les fréquences en dehors de la plage donnée, car les changements de mouvements ne sont pas aussi uniformes que les changements de couleurs. Nous utiliserons donc un filtre de Butterworth dont un descriptif est donné [ici](https://fr.wikipedia.org/wiki/Filtre_de_Butterworth).\n",
    "\n",
    "Dans le papier original, les auteurs proposent d'utiliser un filtre passe-bande de Butterworth du premier ordre, comme dans l'exemple suivant.\n",
    "\n",
    "<center><img src='./Figs/butterworth.png' width=500></center>\n",
    "\n",
    "\n",
    "Filtre de Butterworth avec $w_l = 2 $ et $w_h = 4$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8206d4c",
   "metadata": {},
   "source": [
    "Pour filtrer les images, il faut d'abord calculer les coefficients\n",
    " $b_i$ et $a_i$ du filtre de Butterworth étant donné la bande de fréquence $[w_l, w_h]$ et le frame rate $fps$ de la video comme fréquence temporelle.\n",
    "\n",
    "\n",
    "Pour ce filtrage, vous pourrez utiliser la fonction [`butter`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html) de `scipy`.\n",
    " \n",
    " \n",
    "Vous trouverez ci-dessous la fonction  `butter_bandpass` qui calcule le filtre de Butterworth d'ordre 1 étant donné la bande de fréquence et le frame rate.\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fps, order=1):\n",
    "    '''\n",
    "    Calculates the Butterworth bandpass filter\n",
    "    :param lowcut: low frequency cutoff\n",
    "    :param highcut: high frequency cutoff\n",
    "    :param fs: video frame rate\n",
    "    :param order: filter order - per default = 1\n",
    "    :return: Numerator (b) and denominator (a) polynomials of the IIR filter.\n",
    "    '''\n",
    "\n",
    "    low = lowcut / fps\n",
    "    high = highcut / fps\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e386ec",
   "metadata": {},
   "source": [
    "### 1.8 Application du filtre de Butterworth au Laplacien.\n",
    "\n",
    "Il s'agit maintenant d'appliquer le filtre en suivant l'équation ci-dessous.\n",
    "\n",
    "$$y[n] = \\frac{1}{a_0}(b_{0}x[n] + b_{1}x[n - 1] - a_{1}y[n - 1])$$\n",
    " \n",
    "Avec  $y[n]$ la nième image de la pyramide laplacienne filtrée et $x[n]$ la nième pyramide de Laplace (correspondant à la nième frame)\n",
    "\n",
    "$y[0]$  et $x[0]$ sont les pyramides de Laplace pour la première frame de la vidéo.\n",
    "\n",
    "\n",
    "Ecrire la fonction `apply_filter_butter` qui prend en entrée une séquence d'images et qui applique l'équation précédente, donc le filtre de Butterworth à tous les niveaux d'une pyramide de Laplace d'une séquence d'images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter_butter_to_lapalcian(laplace_video_list, levels, low, high, fps:\n",
    "    '''\n",
    "    Applies the Butterworth filter on video sequence\n",
    "    :param laplace_video_list: Laplace video pyramid\n",
    "    :param levels: Pyramid levels\n",
    "    :param low: Temporal low frequency cutoff\n",
    "    :param high: Temporal high frequency cutoff\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    print('Apply Butterworth filter')\n",
    "    # To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304cf028",
   "metadata": {},
   "source": [
    "### 1.8 Magnification & Reconstruction\n",
    "\n",
    "Dans le cas de la magnification de mouvements, tous les niveaux de la pyramide de Laplacien sont utilisés et donc l'étape de magnification et de reconstruction des mouvements est un peu plus compliquée. Tout d'abord, nous devons adapter dynamiquement le facteur d'amplification pour chaque niveau de la pyramide. Pour ce faire, nous introduisons la longueur d'onde spatiale $\\lambda$ qui représente un niveau donnée de la pyramide de Laplace filtrée \n",
    "\n",
    "$$\\lambda = \\sqrt{h^{2} + w^{2}}$$\n",
    "\n",
    "Avec $h$ et $w$ qui sont respectivement la hauteur et la largeur du niveau considéré. \n",
    "\n",
    "$\\lambda$ et le facteur d'amplification $\\alpha$ sont liés par l'équation suivante : \n",
    "\n",
    "$$(1 + \\alpha)\\delta (t) < \\frac{\\lambda}{8}$$\n",
    "\n",
    "Avec $\\delta (t)$ le facteur de déplacement (se reporter à l'article pour plus de détails).\n",
    "\n",
    "Nous pouvons maintenant introduire la longueur d'onde spatiale de coupure $\\lambda_{c}$\n",
    "\n",
    "$$(1 + \\alpha)\\delta(t) = \\frac{\\lambda_{c}}{8} \\implies \\delta(t) = \\frac{\\frac{\\lambda_{c}}{8}}{1 + \\alpha}$$\n",
    "\n",
    "Le nouveau facteur d'amplication $\\alpha_{\\textrm{new}}$ pour un niveau donnée de la pyramide $F_p$ peut alors être obtenu par \n",
    "\n",
    "$$\\alpha_{\\textrm{new}} = \\frac{\\frac{\\lambda}{8}}{\\delta(t)} - 1$$\n",
    "\n",
    "Le facteur d'amplication est alors pour un niveau $l$ de $F_p$\n",
    "\n",
    "$$\\alpha_{l} = min(\\alpha, \\alpha_{\\textrm{new}})$$\n",
    "\n",
    "De la même manière que pour la magnification de la pyramide de Gaussiennes, nous obtenons notre niveau de pyramide filtrée magnifiée avec \n",
    "\n",
    "$$M_{F_{p}}[l] = \\begin{cases}\\alpha_{l} F_{p}[l]\\ \\text{if Y component}\\\\ \\alpha_{l} A F_{p}[l]\\ \\text{if I or Q component}\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40b1fa",
   "metadata": {},
   "source": [
    "Nous répétons le processus précédent pour tous les niveaux (sauf le premier et le dernier) de $F_p$.\n",
    "\n",
    "Enfin, nous reconstruisons notre image en suréchantillonnant tous les niveaux de $M_{F_{p}}$ aux dimensions de l'image originale, en l'ajoutant à la version YIQ de l'image originale, en convertissant l'image résultante en RVB et en limitant les valeurs de l'image RVB résultante à $[0,255]$. Ce processus est répété pour chaque frame de la vidéo.\n",
    "\n",
    "Compléter la fonction ci-dessous qui permet de mettre en oeuvre ce processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90743150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_butter(laplace_video_list, levels, alpha, cutoff, low, high, fps, width, height, linearAttenuation):\n",
    "    '''\n",
    "    Applies the Butterworth filter on video sequence, magnifies the filtered video sequence\n",
    "    and cuts off spatial frequencies\n",
    "    :param laplace_video_list: Laplace video pyramid\n",
    "    :param levels: Pyramid levels\n",
    "    :param alpha: Magnification factor\n",
    "    :param cutoff: Spatial frequencies cutoff factor\n",
    "    :param low: Temporal low frequency cutoff\n",
    "    :param high: Temporal high frequency cutoff\n",
    "    :param fps: Video frame rate\n",
    "    :param width: Video frame width\n",
    "    :param height: Video frame height\n",
    "    :param linearAttenuation: Boolean if linear attenuation should be applied\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    print('Apply Butterworth filter and magnification' )\n",
    "    filtered_video_list = []\n",
    "    b, a = butter_bandpass(low, high, fps, order=1)\n",
    "\n",
    "    # spatial wavelength lambda\n",
    "    lambda1 = # To complete\n",
    "\n",
    "    delta = # To complete (equation abec la fréquence spatiale de coupure  : cutoff)\n",
    "\n",
    "    for i in range(levels):  # pyramid levels\n",
    "\n",
    "        current_alpha = # To complete  # given in paper\n",
    "\n",
    "\n",
    "        # apply the butterworth filter onto temporal image sequence\n",
    "        # To complete\n",
    "\n",
    "        # spatial frequencies attenuation\n",
    "        # To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c710ff",
   "metadata": {},
   "source": [
    "Ecrire une fonction qui permet de reconstruire une séquence vidéo à partir de la pyramide de Laplaciens filtrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e78797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(filtered_video, levels):\n",
    "    '''\n",
    "    Reconstructs a video sequence from the filtered Laplace video pyramid\n",
    "    :param filtered_video: 2 dimensional video sequence - 1st. index pyramid levels, 2nd. - video frames\n",
    "    :param levels: pyramid levels\n",
    "    :return: video sequence\n",
    "    '''\n",
    "    print('Reconstruct video')\n",
    "\n",
    "    # To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096b17a",
   "metadata": {},
   "source": [
    "Ecrire une fontion qui permet de mettre en place l'amplification de mouvement sur une vidéo complète et sauvegarder la vidéo amplifiée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motionmagnification(vidFile, alpha, cutoff, low, high, chromAttenuation, name):\n",
    "    '''\n",
    "    Performs motion magnification on the video by applying Butterworth bandpass filter and saves the output video\n",
    "    :param vidFile: Video file\n",
    "    :param alpha: Magnification factor\n",
    "    :param cutoff: Spatial frequencies cutoff factor\n",
    "    :param low: Temporal low frequency cutoff\n",
    "    :param high: Temporal high frequency cutoff\n",
    "    :param chromAttenuation: chrominance attenuation \n",
    "    :param name: Output video name\n",
    "    '''\n",
    "    # To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3336d85e",
   "metadata": {},
   "source": [
    "Tester sur les vidéos du répertoire [data](./data). Vous sauvegarderez les vidéos résultats. Ci-dessous un ensemble de valeurs pour les paramètres pour la vidéo baby mais vous pouvez bien evidemment tester différents paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelques paramètres à utiliser\n",
    "\n",
    "# pour la video baby\n",
    "\n",
    "low = 0.4\n",
    "high = 3\n",
    "alpha = 15\n",
    "chromAttenuation = 0.1\n",
    "cutoff =  16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26859a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c78480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc2e878c",
   "metadata": {},
   "source": [
    "# Partie 2 : Générateur d'images Low Poly\n",
    "\n",
    "\n",
    "L'objectif de cette partie est de mettre en place plusieurs approches simples permettant, à partir d'une image source de générer une image artistique dite Low Poly.\n",
    "Ces images ont comme particularité  :\n",
    "\n",
    " + Les objets illustrés sont très abstraits et schématiques. Ils ne respectent pas vraiment la réalité.\n",
    " + Les objets sont illustrés à l'aide de formes basiques, tels que des triangles et des polygones simples.\n",
    " + Une palette de couleur simple est utilisé et il n'y pas beaucoup de contenu fréquentielle ou des textures complexes.\n",
    "\n",
    "Il s'agit ici. de développer des fonctions permettant de transformer une image en une version Low-Poly comme sur l'exemple ci-dessous\n",
    "\n",
    "\n",
    "<center><img src='./Figs/oscar.jpg' width=200></center>\n",
    "\n",
    "\n",
    "<center><img src='./Figs/oscarlowpoly.png' width=200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212eddf1",
   "metadata": {},
   "source": [
    "## 2.1 : Approche à l'aide de points d'intérêts\n",
    "\n",
    "Une première approche consiste à faire en place la chaîne de traitements suivantes :\n",
    "\n",
    " 1. Déterminer des points  (d'interêts ) dans l'image\n",
    " 2. Générer une triangulation de type Delaunay\n",
    " 3. Remplir les triangles à l'aide des couleurs de l'image originale.\n",
    "\n",
    "Il s'agit donc de mettre en place cette première statégie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b188f0e5",
   "metadata": {},
   "source": [
    "Pour l'extraction des points dans l'image, vous comparerez trois stratégies :\n",
    " + La sélection de points de manière aléatoire\n",
    " + La sélection de points sur les contours\n",
    " + la sélection de points d'intérêts (Harris, sift et cie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfcdf0a",
   "metadata": {},
   "source": [
    "### 2.1.1 Extraction de points d'intérêts de manière aléatoire\n",
    "\n",
    "Ecrire une fonction qui prend en entrée une image et qui selectionne de manière aléatoire un ensemble de $n$ points dans une image et qui renvoie cet ensemble de points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ffc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_at_random (img, n):\n",
    "    # TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16345a85",
   "metadata": {},
   "source": [
    "### 2.1.2 Extraction de points d'intérêts sur les contours.\n",
    "\n",
    "Ecrire une fonction qui prend en entrée une image et qui selectionne de manière aléatoire un ensemble de $n$ points sur les contours d'une image donnée. Vous pourrez par exemple prendre pour cela le détecteur de contours de Canny déjà implémenté dans OpenCV : voir [ici](https://docs.opencv.org/4.7.0/da/d22/tutorial_py_canny.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374bf448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_at_edge (img, n):\n",
    "    # TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b840c",
   "metadata": {},
   "source": [
    "### 2.1.3 Extraction de points d'intérêts de manière aléatoire\n",
    "\n",
    "Ecrire une fonction qui prend en entrée une image et qui calcule à partir des fonctions pré-définies dans la bibliothèque OpenCV les points d'intérêts de cette images. Vous pouvez utiliser ici plusieurs approches de votre choix pour l'extraction de ces points d'intérêts comme par exemple les détecteurs sift ou d'autres. Voir [ici](https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc26b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_interest_points (img, n):\n",
    "    # TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1e2db",
   "metadata": {},
   "source": [
    "Ajouter à la liste des points les 4 coins de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e473890",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [[0, 0], [0, h-1], [w-1, 0], [w-1, h-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc1f1b",
   "metadata": {},
   "source": [
    "### 2.1.4 Génération d'une triangulation de Delaunay à partir des points extraits.\n",
    "\n",
    "Il s'agit ici de construire une [triangulation de Delaunay](https://fr.wikipedia.org/wiki/Triangulation_de_Delaunay) à partir des points extraits dans les étapes précédentes.\n",
    "\n",
    "Pour cela, vous pourrez utiliser la bibiothèque `scipy` et le package [`spatial.Delaunay`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Delaunay.html).\n",
    "\n",
    "Le code ci-dessous vous montre un exemple simple d'utilisation de ce package. On voit notamment que les indices des points qui forment les simplexes dans la triangulation sont données dans `_.simplices`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fac5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "points = np.array([[0, 0], [0, 1.1], [1, 0], [1, 1]])\n",
    "from scipy.spatial import Delaunay\n",
    "tri = Delaunay(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e6980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.triplot(points[:,0], points[:,1], tri.simplices)\n",
    "plt.plot(points[:,0], points[:,1], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72581c10",
   "metadata": {},
   "source": [
    "Ecrire une fonction qui à partir d'un ensemble de points extraits d'une image contruit la triangulation de Delaunay de cet ensemble de points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c7690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delaunay (image):\n",
    "    # TO COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75844e4",
   "metadata": {},
   "source": [
    "### 2.1.5  Remplir les triangles à l'aide des couleurs de l'image originale\n",
    "\n",
    "Ici il s'agit d'assigner chaque pixel à un triangle de la triangulation de Delaunay et de faire la moyenne des couleurs dans ce triangle pour obtenir l'image Low Poly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colour_of_tri(tri, image):\n",
    "    # assign each pixel to a triangle, and then take all the pixels in a triangle and average the colour and return the average color\n",
    "    \n",
    "    # TO Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55c9d8",
   "metadata": {},
   "source": [
    "Ecrire la fonction permettant de générer l'image Low Poly. Il s'agit donc de tracer les triangles et de donner la couleur moyenne des pixels à l'intérieur de chaque triangle. Les fonctions [`cv2.line`](https://docs.opencv.org/4.x/d6/d6e/group__imgproc__draw.html) et [`find_simplex`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Delaunay.find_simplex.html) pourraient vous être utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_low_poly_img (img):\n",
    "    # To complete\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ac7bc",
   "metadata": {},
   "source": [
    "Tester vos différentes approches sur les images du répertoire [lowpoly](./lowpoly) et afficher les images originales et les images obtenues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ff817",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2cb71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
